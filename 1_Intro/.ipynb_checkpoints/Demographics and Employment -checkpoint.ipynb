{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_demo=pd.read_csv('CPSData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PeopleInHousehold</th>\n",
       "      <th>Region</th>\n",
       "      <th>State</th>\n",
       "      <th>MetroAreaCode</th>\n",
       "      <th>Age</th>\n",
       "      <th>Married</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Education</th>\n",
       "      <th>Race</th>\n",
       "      <th>Hispanic</th>\n",
       "      <th>CountryOfBirthCode</th>\n",
       "      <th>Citizenship</th>\n",
       "      <th>EmploymentStatus</th>\n",
       "      <th>Industry</th>\n",
       "      <th>is_US_citizen</th>\n",
       "      <th>has_NAN</th>\n",
       "      <th>NAN_Married</th>\n",
       "      <th>Missing_MetroAreCode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>South</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>26620.0</td>\n",
       "      <td>85</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Female</td>\n",
       "      <td>Associate degree</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>Citizen, Native</td>\n",
       "      <td>Retired</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>South</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>13820.0</td>\n",
       "      <td>21</td>\n",
       "      <td>Never Married</td>\n",
       "      <td>Male</td>\n",
       "      <td>High school</td>\n",
       "      <td>Black</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>Citizen, Native</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>Professional and business services</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>South</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>13820.0</td>\n",
       "      <td>37</td>\n",
       "      <td>Never Married</td>\n",
       "      <td>Female</td>\n",
       "      <td>High school</td>\n",
       "      <td>Black</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>Citizen, Native</td>\n",
       "      <td>Disabled</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>South</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>13820.0</td>\n",
       "      <td>18</td>\n",
       "      <td>Never Married</td>\n",
       "      <td>Male</td>\n",
       "      <td>No high school diploma</td>\n",
       "      <td>Black</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>Citizen, Native</td>\n",
       "      <td>Not in Labor Force</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>South</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>26620.0</td>\n",
       "      <td>52</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Female</td>\n",
       "      <td>Associate degree</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>Citizen, Native</td>\n",
       "      <td>Employed</td>\n",
       "      <td>Professional and business services</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PeopleInHousehold Region    State  MetroAreaCode  Age        Married  \\\n",
       "0                  1  South  Alabama        26620.0   85        Widowed   \n",
       "1                  3  South  Alabama        13820.0   21  Never Married   \n",
       "2                  3  South  Alabama        13820.0   37  Never Married   \n",
       "3                  3  South  Alabama        13820.0   18  Never Married   \n",
       "4                  3  South  Alabama        26620.0   52        Widowed   \n",
       "\n",
       "      Sex               Education   Race  Hispanic  CountryOfBirthCode  \\\n",
       "0  Female        Associate degree  White         0                  57   \n",
       "1    Male             High school  Black         0                  57   \n",
       "2  Female             High school  Black         0                  57   \n",
       "3    Male  No high school diploma  Black         0                  57   \n",
       "4  Female        Associate degree  White         0                  57   \n",
       "\n",
       "       Citizenship    EmploymentStatus                            Industry  \\\n",
       "0  Citizen, Native             Retired                                 NaN   \n",
       "1  Citizen, Native          Unemployed  Professional and business services   \n",
       "2  Citizen, Native            Disabled                                 NaN   \n",
       "3  Citizen, Native  Not in Labor Force                                 NaN   \n",
       "4  Citizen, Native            Employed  Professional and business services   \n",
       "\n",
       "  is_US_citizen has_NAN NAN_Married Missing_MetroAreCode  \n",
       "0          True    True       False                False  \n",
       "1          True   False       False                False  \n",
       "2          True    True       False                False  \n",
       "3          True    True       False                False  \n",
       "4          True   False       False                False  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "High school                30906\n",
       "NaN                        25338\n",
       "Bachelor's degree          19443\n",
       "Some college, no degree    18863\n",
       "No high school diploma     16095\n",
       "Associate degree            9913\n",
       "Master's degree             7816\n",
       "Doctorate degree            1516\n",
       "Professional degree         1412\n",
       "Name: Education, dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demo.Education.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 1.1 - Loading and Summarizing the Dataset\n",
    "1 point possible (graded)\n",
    "Load the dataset from CPSData.csv into a data frame called CPS, and view the dataset with the summary() and str() commands.\n",
    "\n",
    "Explanation\n",
    "You can load the data with:\n",
    "CPS = read.csv(\"CPSData.csv\")\n",
    "How many interviewees are in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131302"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demo.PeopleInHousehold.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 1.2 - Loading and Summarizing the Dataset\n",
    "1 point possible (graded)\n",
    "Among the interviewees with a value reported for the Industry variable, what is the most common industry of employment? Please enter the name exactly how you see it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Educational and health services'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demo.Industry.value_counts().index[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 1.3 - Loading and Summarizing the Dataset\n",
    "2 points possible (graded)\n",
    "Recall from the homework assignment \"The Analytical Detective\" that you can call the sort() function on the output of the table() function to obtain a sorted breakdown of a variable. For instance, sort(table(CPS$Region)) sorts the regions by the number of interviewees from that region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "California\n",
      "New Mexico\n"
     ]
    }
   ],
   "source": [
    "print df_demo.State.value_counts().index[0]\n",
    "print df_demo.State.value_counts().index[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 1.4 - Loading and Summarizing the Dataset\n",
    "1 point possible (graded)\n",
    "What proportion of interviewees are citizens of the United States?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_demo['is_US_citizen']=~df_demo.Citizenship.str.contains('Non')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94219433062710389"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demo.is_US_citizen.value_counts()[1]*1.0/df_demo.is_US_citizen.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 1.5 - Loading and Summarizing the Dataset\n",
    "1 point possible (graded)\n",
    "The CPS differentiates between race (with possible values American Indian, Asian, Black, Pacific Islander, White, or Multiracial) and ethnicity. A number of interviewees are of Hispanic ethnicity, as captured by the Hispanic variable. For which races are there at least 250 interviewees in the CPS dataset of Hispanic ethnicity? (Select all that apply.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>PeopleInHousehold</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hispanic</th>\n",
       "      <th>Race</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">0</th>\n",
       "      <th>American Indian</th>\n",
       "      <td>1129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Asian</th>\n",
       "      <td>6407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Black</th>\n",
       "      <td>13292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multiracial</th>\n",
       "      <td>2449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pacific Islander</th>\n",
       "      <td>541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>White</th>\n",
       "      <td>89190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">1</th>\n",
       "      <th>American Indian</th>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Asian</th>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Black</th>\n",
       "      <td>621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multiracial</th>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pacific Islander</th>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>White</th>\n",
       "      <td>16731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           PeopleInHousehold\n",
       "Hispanic Race                               \n",
       "0        American Indian                1129\n",
       "         Asian                          6407\n",
       "         Black                         13292\n",
       "         Multiracial                    2449\n",
       "         Pacific Islander                541\n",
       "         White                         89190\n",
       "1        American Indian                 304\n",
       "         Asian                           113\n",
       "         Black                           621\n",
       "         Multiracial                     448\n",
       "         Pacific Islander                 77\n",
       "         White                         16731"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demo.groupby(['Hispanic','Race'])[['PeopleInHousehold']].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2.1 - Evaluating Missing Values\n",
    "1 point possible (graded)\n",
    "Which variables have at least one interviewee with a missing (NA) value? (Select all that apply.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MetroAreaCode\n",
      "Married\n",
      "Education\n",
      "EmploymentStatus\n",
      "Industry\n"
     ]
    }
   ],
   "source": [
    "for col in df_demo.columns.tolist():\n",
    "    if df_demo[col].isnull().any():\n",
    "        print col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_demo['has_NAN']=df_demo.apply(lambda x: x.isnull().any(),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     81906\n",
       "False    49396\n",
       "Name: has_NAN, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demo.has_NAN.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2.2 - Evaluating Missing Values\n",
    "1 point possible (graded)\n",
    "Often when evaluating a new dataset, we try to identify if there is a pattern in the missing values in the dataset. We will try to determine if there is a pattern in the missing values of the Married variable. The function is.na(CPS$Married) returns a vector of TRUE/FALSE values for whether the Married variable is missing. We can see the breakdown of whether Married is missing based on the reported value of the Region variable with the function table(CPS$Region, is.na(CPS$Married)). Which is the most accurate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_demo['NAN_Married'] =df_demo.Married.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NAN_Married  Age\n",
       "False        15     1795\n",
       "             16     1751\n",
       "             17     1764\n",
       "             18     1596\n",
       "             19     1517\n",
       "             20     1398\n",
       "             21     1525\n",
       "             22     1536\n",
       "             23     1638\n",
       "             24     1627\n",
       "             25     1604\n",
       "             26     1643\n",
       "             27     1657\n",
       "             28     1736\n",
       "             29     1645\n",
       "             30     1854\n",
       "             31     1762\n",
       "             32     1790\n",
       "             33     1804\n",
       "             34     1653\n",
       "             35     1716\n",
       "             36     1663\n",
       "             37     1531\n",
       "             38     1530\n",
       "             39     1542\n",
       "             40     1571\n",
       "             41     1673\n",
       "             42     1711\n",
       "             43     1819\n",
       "             44     1764\n",
       "                    ... \n",
       "             67     1227\n",
       "             68     1130\n",
       "             69     1062\n",
       "             70     1195\n",
       "             71     1031\n",
       "             72      941\n",
       "             73      896\n",
       "             74      842\n",
       "             75      763\n",
       "             76      729\n",
       "             77      698\n",
       "             78      659\n",
       "             79      661\n",
       "             80     2664\n",
       "             85     2446\n",
       "True         0      1283\n",
       "             1      1559\n",
       "             2      1574\n",
       "             3      1693\n",
       "             4      1695\n",
       "             5      1795\n",
       "             6      1721\n",
       "             7      1681\n",
       "             8      1729\n",
       "             9      1748\n",
       "             10     1750\n",
       "             11     1721\n",
       "             12     1797\n",
       "             13     1802\n",
       "             14     1790\n",
       "Name: PeopleInHousehold, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demo.groupby(['NAN_Married','Age']).count().PeopleInHousehold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2.3 - Evaluating Missing Values\n",
    "2 points possible (graded)\n",
    "As mentioned in the variable descriptions, MetroAreaCode is missing if an interviewee does not live in a metropolitan area. Using the same technique as in the previous question, answer the following questions about people who live in non-metropolitan areas.\n",
    "\n",
    "How many states had all interviewees living in a non-metropolitan area (aka they have a missing MetroAreaCode value)? For this question, treat the District of Columbia as a state (even though it is not technically a state).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_demo['Missing_MetroAreCode']=df_demo.MetroAreaCode.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_t = df_demo.groupby(['State','Missing_MetroAreCode']).count().unstack('Missing_MetroAreCode')[['PeopleInHousehold']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_t.columns=df_t.columns.droplevel(level=0)\n",
    "df_t.columns=['No_metro_code', 'has_metro_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No_metro_code</th>\n",
       "      <th>has_metro_code</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Montana</th>\n",
       "      <td>199.0</td>\n",
       "      <td>1015.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>West Virginia</th>\n",
       "      <td>344.0</td>\n",
       "      <td>1065.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mississippi</th>\n",
       "      <td>376.0</td>\n",
       "      <td>854.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>North Dakota</th>\n",
       "      <td>432.0</td>\n",
       "      <td>1213.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>South Dakota</th>\n",
       "      <td>595.0</td>\n",
       "      <td>1405.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vermont</th>\n",
       "      <td>657.0</td>\n",
       "      <td>1233.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arkansas</th>\n",
       "      <td>724.0</td>\n",
       "      <td>697.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Idaho</th>\n",
       "      <td>761.0</td>\n",
       "      <td>757.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nebraska</th>\n",
       "      <td>816.0</td>\n",
       "      <td>1133.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Mexico</th>\n",
       "      <td>832.0</td>\n",
       "      <td>270.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kentucky</th>\n",
       "      <td>908.0</td>\n",
       "      <td>933.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Maine</th>\n",
       "      <td>909.0</td>\n",
       "      <td>1354.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alabama</th>\n",
       "      <td>1020.0</td>\n",
       "      <td>356.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oklahoma</th>\n",
       "      <td>1024.0</td>\n",
       "      <td>499.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>South Carolina</th>\n",
       "      <td>1139.0</td>\n",
       "      <td>519.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Hampshire</th>\n",
       "      <td>1148.0</td>\n",
       "      <td>1514.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tennessee</th>\n",
       "      <td>1149.0</td>\n",
       "      <td>635.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Louisiana</th>\n",
       "      <td>1216.0</td>\n",
       "      <td>234.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kansas</th>\n",
       "      <td>1234.0</td>\n",
       "      <td>701.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iowa</th>\n",
       "      <td>1297.0</td>\n",
       "      <td>1231.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arizona</th>\n",
       "      <td>1327.0</td>\n",
       "      <td>201.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indiana</th>\n",
       "      <td>1420.0</td>\n",
       "      <td>584.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Missouri</th>\n",
       "      <td>1440.0</td>\n",
       "      <td>705.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Utah</th>\n",
       "      <td>1455.0</td>\n",
       "      <td>387.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oregon</th>\n",
       "      <td>1519.0</td>\n",
       "      <td>424.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hawaii</th>\n",
       "      <td>1576.0</td>\n",
       "      <td>523.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nevada</th>\n",
       "      <td>1609.0</td>\n",
       "      <td>247.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>North Carolina</th>\n",
       "      <td>1642.0</td>\n",
       "      <td>977.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Delaware</th>\n",
       "      <td>1696.0</td>\n",
       "      <td>518.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>District of Columbia</th>\n",
       "      <td>1791.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Massachusetts</th>\n",
       "      <td>1858.0</td>\n",
       "      <td>129.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wisconsin</th>\n",
       "      <td>1882.0</td>\n",
       "      <td>804.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Washington</th>\n",
       "      <td>1937.0</td>\n",
       "      <td>429.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Minnesota</th>\n",
       "      <td>2150.0</td>\n",
       "      <td>989.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rhode Island</th>\n",
       "      <td>2209.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Georgia</th>\n",
       "      <td>2250.0</td>\n",
       "      <td>557.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Virginia</th>\n",
       "      <td>2367.0</td>\n",
       "      <td>586.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Michigan</th>\n",
       "      <td>2517.0</td>\n",
       "      <td>546.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Colorado</th>\n",
       "      <td>2545.0</td>\n",
       "      <td>380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Jersey</th>\n",
       "      <td>2567.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Connecticut</th>\n",
       "      <td>2593.0</td>\n",
       "      <td>243.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ohio</th>\n",
       "      <td>2754.0</td>\n",
       "      <td>924.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Maryland</th>\n",
       "      <td>2978.0</td>\n",
       "      <td>222.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pennsylvania</th>\n",
       "      <td>3245.0</td>\n",
       "      <td>685.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Illinois</th>\n",
       "      <td>3473.0</td>\n",
       "      <td>439.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Florida</th>\n",
       "      <td>4947.0</td>\n",
       "      <td>202.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New York</th>\n",
       "      <td>5144.0</td>\n",
       "      <td>451.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Texas</th>\n",
       "      <td>6060.0</td>\n",
       "      <td>1017.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>California</th>\n",
       "      <td>11333.0</td>\n",
       "      <td>237.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alaska</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1590.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wyoming</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1624.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      No_metro_code  has_metro_code\n",
       "State                                              \n",
       "Montana                       199.0          1015.0\n",
       "West Virginia                 344.0          1065.0\n",
       "Mississippi                   376.0           854.0\n",
       "North Dakota                  432.0          1213.0\n",
       "South Dakota                  595.0          1405.0\n",
       "Vermont                       657.0          1233.0\n",
       "Arkansas                      724.0           697.0\n",
       "Idaho                         761.0           757.0\n",
       "Nebraska                      816.0          1133.0\n",
       "New Mexico                    832.0           270.0\n",
       "Kentucky                      908.0           933.0\n",
       "Maine                         909.0          1354.0\n",
       "Alabama                      1020.0           356.0\n",
       "Oklahoma                     1024.0           499.0\n",
       "South Carolina               1139.0           519.0\n",
       "New Hampshire                1148.0          1514.0\n",
       "Tennessee                    1149.0           635.0\n",
       "Louisiana                    1216.0           234.0\n",
       "Kansas                       1234.0           701.0\n",
       "Iowa                         1297.0          1231.0\n",
       "Arizona                      1327.0           201.0\n",
       "Indiana                      1420.0           584.0\n",
       "Missouri                     1440.0           705.0\n",
       "Utah                         1455.0           387.0\n",
       "Oregon                       1519.0           424.0\n",
       "Hawaii                       1576.0           523.0\n",
       "Nevada                       1609.0           247.0\n",
       "North Carolina               1642.0           977.0\n",
       "Delaware                     1696.0           518.0\n",
       "District of Columbia         1791.0             NaN\n",
       "Massachusetts                1858.0           129.0\n",
       "Wisconsin                    1882.0           804.0\n",
       "Washington                   1937.0           429.0\n",
       "Minnesota                    2150.0           989.0\n",
       "Rhode Island                 2209.0             NaN\n",
       "Georgia                      2250.0           557.0\n",
       "Virginia                     2367.0           586.0\n",
       "Michigan                     2517.0           546.0\n",
       "Colorado                     2545.0           380.0\n",
       "New Jersey                   2567.0             NaN\n",
       "Connecticut                  2593.0           243.0\n",
       "Ohio                         2754.0           924.0\n",
       "Maryland                     2978.0           222.0\n",
       "Pennsylvania                 3245.0           685.0\n",
       "Illinois                     3473.0           439.0\n",
       "Florida                      4947.0           202.0\n",
       "New York                     5144.0           451.0\n",
       "Texas                        6060.0          1017.0\n",
       "California                  11333.0           237.0\n",
       "Alaska                          NaN          1590.0\n",
       "Wyoming                         NaN          1624.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t.sort_values(by='No_metro_code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2.4 - Evaluating Missing Values\n",
    "1 point possible (graded)\n",
    "Which region of the United States has the largest proportion of interviewees living in a non-metropolitan area?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area_Code</th>\n",
       "      <th>No_Code</th>\n",
       "      <th>percentage Area_Code</th>\n",
       "      <th>percentage No Metro Area Code</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Region</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Midwest</th>\n",
       "      <td>20010</td>\n",
       "      <td>10674</td>\n",
       "      <td>65.21</td>\n",
       "      <td>34.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Northeast</th>\n",
       "      <td>20330</td>\n",
       "      <td>5609</td>\n",
       "      <td>78.38</td>\n",
       "      <td>21.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>South</th>\n",
       "      <td>31631</td>\n",
       "      <td>9871</td>\n",
       "      <td>76.22</td>\n",
       "      <td>23.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>West</th>\n",
       "      <td>25093</td>\n",
       "      <td>8084</td>\n",
       "      <td>75.63</td>\n",
       "      <td>24.31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Area_Code  No_Code  percentage Area_Code  \\\n",
       "Region                                                \n",
       "Midwest        20010    10674                 65.21   \n",
       "Northeast      20330     5609                 78.38   \n",
       "South          31631     9871                 76.22   \n",
       "West           25093     8084                 75.63   \n",
       "\n",
       "           percentage No Metro Area Code   \n",
       "Region                                     \n",
       "Midwest                             34.71  \n",
       "Northeast                           21.56  \n",
       "South                               23.74  \n",
       "West                                24.31  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_regions_no_metroCode = pd.DataFrame(df_demo.groupby(['Region','Missing_MetroAreCode']).count().PeopleInHousehold)\n",
    "\n",
    "df_regions_no_metroCode = df_regions_no_metroCode.unstack('Missing_MetroAreCode')\n",
    "df_regions_no_metroCode.columns = df_regions_no_metroCode.columns.droplevel(level=0)\n",
    "df_regions_no_metroCode.columns =['Area_Code','No_Code']\n",
    "df_regions_no_metroCode['percentage Area_Code']=np.round(100.0*df_regions_no_metroCode['Area_Code']/df_regions_no_metroCode.sum(axis=1),2)\n",
    "df_regions_no_metroCode['percentage No Metro Area Code ']=np.round(100.0*df_regions_no_metroCode['No_Code']/df_regions_no_metroCode.sum(axis=1),2)\n",
    "\n",
    "df_regions_no_metroCode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2.5 - Evaluating Missing Values\n",
    "4.0 points possible (graded)\n",
    "While we were able to use the table() command to compute the proportion of interviewees from each region not living in a metropolitan area, it was somewhat tedious (it involved manually computing the proportion for each region) and isn't something you would want to do if there were a larger number of options. It turns out there is a less tedious way to compute the proportion of values that are TRUE. The mean() function, which takes the average of the values passed to it, will treat TRUE as 1 and FALSE as 0, meaning it returns the proportion of values that are true. For instance, mean(c(TRUE, FALSE, TRUE, TRUE)) returns 0.75. Knowing this, use tapply() with the mean function to answer the following questions:\n",
    "\n",
    "Which state has a proportion of interviewees living in a non-metropolitan area closest to 30%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area_Code</th>\n",
       "      <th>No_Code</th>\n",
       "      <th>percentage Area_Code</th>\n",
       "      <th>percentage No Metro Area Code</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>California</th>\n",
       "      <td>11333.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>97.95</td>\n",
       "      <td>2.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Florida</th>\n",
       "      <td>4947.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>96.08</td>\n",
       "      <td>3.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Massachusetts</th>\n",
       "      <td>1858.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>93.51</td>\n",
       "      <td>6.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Maryland</th>\n",
       "      <td>2978.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>93.06</td>\n",
       "      <td>6.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New York</th>\n",
       "      <td>5144.0</td>\n",
       "      <td>451.0</td>\n",
       "      <td>91.94</td>\n",
       "      <td>7.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Connecticut</th>\n",
       "      <td>2593.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>91.43</td>\n",
       "      <td>8.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Illinois</th>\n",
       "      <td>3473.0</td>\n",
       "      <td>439.0</td>\n",
       "      <td>88.78</td>\n",
       "      <td>10.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arizona</th>\n",
       "      <td>1327.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>86.85</td>\n",
       "      <td>12.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Colorado</th>\n",
       "      <td>2545.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>87.01</td>\n",
       "      <td>12.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nevada</th>\n",
       "      <td>1609.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>86.69</td>\n",
       "      <td>12.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Texas</th>\n",
       "      <td>6060.0</td>\n",
       "      <td>1017.0</td>\n",
       "      <td>85.63</td>\n",
       "      <td>14.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Louisiana</th>\n",
       "      <td>1216.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>83.86</td>\n",
       "      <td>15.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pennsylvania</th>\n",
       "      <td>3245.0</td>\n",
       "      <td>685.0</td>\n",
       "      <td>82.57</td>\n",
       "      <td>17.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Michigan</th>\n",
       "      <td>2517.0</td>\n",
       "      <td>546.0</td>\n",
       "      <td>82.17</td>\n",
       "      <td>17.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Washington</th>\n",
       "      <td>1937.0</td>\n",
       "      <td>429.0</td>\n",
       "      <td>81.87</td>\n",
       "      <td>17.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Georgia</th>\n",
       "      <td>2250.0</td>\n",
       "      <td>557.0</td>\n",
       "      <td>80.16</td>\n",
       "      <td>19.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Virginia</th>\n",
       "      <td>2367.0</td>\n",
       "      <td>586.0</td>\n",
       "      <td>80.16</td>\n",
       "      <td>19.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Utah</th>\n",
       "      <td>1455.0</td>\n",
       "      <td>387.0</td>\n",
       "      <td>78.99</td>\n",
       "      <td>20.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oregon</th>\n",
       "      <td>1519.0</td>\n",
       "      <td>424.0</td>\n",
       "      <td>78.18</td>\n",
       "      <td>20.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Delaware</th>\n",
       "      <td>1696.0</td>\n",
       "      <td>518.0</td>\n",
       "      <td>76.60</td>\n",
       "      <td>22.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Mexico</th>\n",
       "      <td>832.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>75.50</td>\n",
       "      <td>22.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hawaii</th>\n",
       "      <td>1576.0</td>\n",
       "      <td>523.0</td>\n",
       "      <td>75.08</td>\n",
       "      <td>24.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alabama</th>\n",
       "      <td>1020.0</td>\n",
       "      <td>356.0</td>\n",
       "      <td>74.13</td>\n",
       "      <td>24.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ohio</th>\n",
       "      <td>2754.0</td>\n",
       "      <td>924.0</td>\n",
       "      <td>74.88</td>\n",
       "      <td>24.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indiana</th>\n",
       "      <td>1420.0</td>\n",
       "      <td>584.0</td>\n",
       "      <td>70.86</td>\n",
       "      <td>28.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wisconsin</th>\n",
       "      <td>1882.0</td>\n",
       "      <td>804.0</td>\n",
       "      <td>70.07</td>\n",
       "      <td>29.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>South Carolina</th>\n",
       "      <td>1139.0</td>\n",
       "      <td>519.0</td>\n",
       "      <td>68.70</td>\n",
       "      <td>30.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Minnesota</th>\n",
       "      <td>2150.0</td>\n",
       "      <td>989.0</td>\n",
       "      <td>68.49</td>\n",
       "      <td>30.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oklahoma</th>\n",
       "      <td>1024.0</td>\n",
       "      <td>499.0</td>\n",
       "      <td>67.24</td>\n",
       "      <td>31.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Missouri</th>\n",
       "      <td>1440.0</td>\n",
       "      <td>705.0</td>\n",
       "      <td>67.13</td>\n",
       "      <td>31.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tennessee</th>\n",
       "      <td>1149.0</td>\n",
       "      <td>635.0</td>\n",
       "      <td>64.41</td>\n",
       "      <td>34.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kansas</th>\n",
       "      <td>1234.0</td>\n",
       "      <td>701.0</td>\n",
       "      <td>63.77</td>\n",
       "      <td>35.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>North Carolina</th>\n",
       "      <td>1642.0</td>\n",
       "      <td>977.0</td>\n",
       "      <td>62.70</td>\n",
       "      <td>36.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arkansas</th>\n",
       "      <td>724.0</td>\n",
       "      <td>697.0</td>\n",
       "      <td>50.95</td>\n",
       "      <td>47.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iowa</th>\n",
       "      <td>1297.0</td>\n",
       "      <td>1231.0</td>\n",
       "      <td>51.31</td>\n",
       "      <td>47.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Idaho</th>\n",
       "      <td>761.0</td>\n",
       "      <td>757.0</td>\n",
       "      <td>50.13</td>\n",
       "      <td>48.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kentucky</th>\n",
       "      <td>908.0</td>\n",
       "      <td>933.0</td>\n",
       "      <td>49.32</td>\n",
       "      <td>49.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Hampshire</th>\n",
       "      <td>1148.0</td>\n",
       "      <td>1514.0</td>\n",
       "      <td>43.13</td>\n",
       "      <td>55.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nebraska</th>\n",
       "      <td>816.0</td>\n",
       "      <td>1133.0</td>\n",
       "      <td>41.87</td>\n",
       "      <td>56.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Maine</th>\n",
       "      <td>909.0</td>\n",
       "      <td>1354.0</td>\n",
       "      <td>40.17</td>\n",
       "      <td>58.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vermont</th>\n",
       "      <td>657.0</td>\n",
       "      <td>1233.0</td>\n",
       "      <td>34.76</td>\n",
       "      <td>64.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mississippi</th>\n",
       "      <td>376.0</td>\n",
       "      <td>854.0</td>\n",
       "      <td>30.57</td>\n",
       "      <td>67.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>South Dakota</th>\n",
       "      <td>595.0</td>\n",
       "      <td>1405.0</td>\n",
       "      <td>29.75</td>\n",
       "      <td>69.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>North Dakota</th>\n",
       "      <td>432.0</td>\n",
       "      <td>1213.0</td>\n",
       "      <td>26.26</td>\n",
       "      <td>72.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>West Virginia</th>\n",
       "      <td>344.0</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>24.41</td>\n",
       "      <td>74.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Montana</th>\n",
       "      <td>199.0</td>\n",
       "      <td>1015.0</td>\n",
       "      <td>16.39</td>\n",
       "      <td>82.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alaska</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1590.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wyoming</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1624.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>District of Columbia</th>\n",
       "      <td>1791.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Jersey</th>\n",
       "      <td>2567.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rhode Island</th>\n",
       "      <td>2209.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Area_Code  No_Code  percentage Area_Code  \\\n",
       "State                                                            \n",
       "California              11333.0    237.0                 97.95   \n",
       "Florida                  4947.0    202.0                 96.08   \n",
       "Massachusetts            1858.0    129.0                 93.51   \n",
       "Maryland                 2978.0    222.0                 93.06   \n",
       "New York                 5144.0    451.0                 91.94   \n",
       "Connecticut              2593.0    243.0                 91.43   \n",
       "Illinois                 3473.0    439.0                 88.78   \n",
       "Arizona                  1327.0    201.0                 86.85   \n",
       "Colorado                 2545.0    380.0                 87.01   \n",
       "Nevada                   1609.0    247.0                 86.69   \n",
       "Texas                    6060.0   1017.0                 85.63   \n",
       "Louisiana                1216.0    234.0                 83.86   \n",
       "Pennsylvania             3245.0    685.0                 82.57   \n",
       "Michigan                 2517.0    546.0                 82.17   \n",
       "Washington               1937.0    429.0                 81.87   \n",
       "Georgia                  2250.0    557.0                 80.16   \n",
       "Virginia                 2367.0    586.0                 80.16   \n",
       "Utah                     1455.0    387.0                 78.99   \n",
       "Oregon                   1519.0    424.0                 78.18   \n",
       "Delaware                 1696.0    518.0                 76.60   \n",
       "New Mexico                832.0    270.0                 75.50   \n",
       "Hawaii                   1576.0    523.0                 75.08   \n",
       "Alabama                  1020.0    356.0                 74.13   \n",
       "Ohio                     2754.0    924.0                 74.88   \n",
       "Indiana                  1420.0    584.0                 70.86   \n",
       "Wisconsin                1882.0    804.0                 70.07   \n",
       "South Carolina           1139.0    519.0                 68.70   \n",
       "Minnesota                2150.0    989.0                 68.49   \n",
       "Oklahoma                 1024.0    499.0                 67.24   \n",
       "Missouri                 1440.0    705.0                 67.13   \n",
       "Tennessee                1149.0    635.0                 64.41   \n",
       "Kansas                   1234.0    701.0                 63.77   \n",
       "North Carolina           1642.0    977.0                 62.70   \n",
       "Arkansas                  724.0    697.0                 50.95   \n",
       "Iowa                     1297.0   1231.0                 51.31   \n",
       "Idaho                     761.0    757.0                 50.13   \n",
       "Kentucky                  908.0    933.0                 49.32   \n",
       "New Hampshire            1148.0   1514.0                 43.13   \n",
       "Nebraska                  816.0   1133.0                 41.87   \n",
       "Maine                     909.0   1354.0                 40.17   \n",
       "Vermont                   657.0   1233.0                 34.76   \n",
       "Mississippi               376.0    854.0                 30.57   \n",
       "South Dakota              595.0   1405.0                 29.75   \n",
       "North Dakota              432.0   1213.0                 26.26   \n",
       "West Virginia             344.0   1065.0                 24.41   \n",
       "Montana                   199.0   1015.0                 16.39   \n",
       "Alaska                      NaN   1590.0                   NaN   \n",
       "Wyoming                     NaN   1624.0                   NaN   \n",
       "District of Columbia     1791.0      NaN                100.00   \n",
       "New Jersey               2567.0      NaN                100.00   \n",
       "Rhode Island             2209.0      NaN                100.00   \n",
       "\n",
       "                      percentage No Metro Area Code  \n",
       "State                                                \n",
       "California                                     2.03  \n",
       "Florida                                        3.85  \n",
       "Massachusetts                                  6.20  \n",
       "Maryland                                       6.74  \n",
       "New York                                       7.93  \n",
       "Connecticut                                    8.30  \n",
       "Illinois                                      10.97  \n",
       "Arizona                                       12.45  \n",
       "Colorado                                      12.62  \n",
       "Nevada                                        12.71  \n",
       "Texas                                         14.20  \n",
       "Louisiana                                     15.26  \n",
       "Pennsylvania                                  17.07  \n",
       "Michigan                                      17.36  \n",
       "Washington                                    17.53  \n",
       "Georgia                                       19.29  \n",
       "Virginia                                      19.32  \n",
       "Utah                                          20.15  \n",
       "Oregon                                        20.98  \n",
       "Delaware                                      22.61  \n",
       "New Mexico                                    22.93  \n",
       "Hawaii                                        24.06  \n",
       "Alabama                                       24.55  \n",
       "Ohio                                          24.62  \n",
       "Indiana                                       28.15  \n",
       "Wisconsin                                     29.17  \n",
       "South Carolina                                30.06  \n",
       "Minnesota                                     30.83  \n",
       "Oklahoma                                      31.38  \n",
       "Missouri                                      31.87  \n",
       "Tennessee                                     34.35  \n",
       "Kansas                                        35.07  \n",
       "North Carolina                                36.43  \n",
       "Arkansas                                      47.35  \n",
       "Iowa                                          47.73  \n",
       "Idaho                                         48.27  \n",
       "Kentucky                                      49.36  \n",
       "New Hampshire                                 55.97  \n",
       "Nebraska                                      56.91  \n",
       "Maine                                         58.79  \n",
       "Vermont                                       64.06  \n",
       "Mississippi                                   67.75  \n",
       "South Dakota                                  69.22  \n",
       "North Dakota                                  72.58  \n",
       "West Virginia                                 74.30  \n",
       "Montana                                       82.49  \n",
       "Alaska                                       100.00  \n",
       "Wyoming                                      100.00  \n",
       "District of Columbia                            NaN  \n",
       "New Jersey                                      NaN  \n",
       "Rhode Island                                    NaN  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_states_no_metroCode = pd.DataFrame(df_demo.groupby(['State','Missing_MetroAreCode']).count().PeopleInHousehold)\n",
    "\n",
    "df_states_no_metroCode = df_states_no_metroCode.unstack('Missing_MetroAreCode')\n",
    "df_states_no_metroCode.columns = df_states_no_metroCode.columns.droplevel(level=0)\n",
    "df_states_no_metroCode.columns =['Area_Code','No_Code']\n",
    "df_states_no_metroCode['percentage Area_Code']=np.round(100.0*df_states_no_metroCode['Area_Code']/df_states_no_metroCode.sum(axis=1),2)\n",
    "df_states_no_metroCode['percentage No Metro Area Code']=np.round(100.0*df_states_no_metroCode['No_Code']/df_states_no_metroCode.sum(axis=1),2)\n",
    "\n",
    "df_states_no_metroCode.sort_values(by='percentage No Metro Area Code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 3.1 - Integrating Metropolitan Area Data\n",
    "2 points possible (graded)\n",
    "Codes like MetroAreaCode and CountryOfBirthCode are a compact way to encode factor variables with text as their possible values, and they are therefore quite common in survey datasets. In fact, all but one of the variables in this dataset were actually stored by a numeric code in the original CPS datafile.\n",
    "\n",
    "When analyzing a variable stored by a numeric code, we will often want to convert it into the values the codes represent. To do this, we will use a dictionary, which maps the the code to the actual value of the variable. We have provided dictionaries MetroAreaCodes.csv and CountryCodes.csv, which respectively map MetroAreaCode and CountryOfBirthCode into their true values. Read these two dictionaries into data frames MetroAreaMap and CountryMap.\n",
    "\n",
    "How many observations (codes for metropolitan areas) are there in MetroAreaMap?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code</th>\n",
       "      <th>MetroArea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>460</td>\n",
       "      <td>Appleton-Oshkosh-Neenah, WI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3000</td>\n",
       "      <td>Grand Rapids-Muskegon-Holland, MI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3160</td>\n",
       "      <td>Greenville-Spartanburg-Anderson, SC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3610</td>\n",
       "      <td>Jamestown, NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3720</td>\n",
       "      <td>Kalamazoo-Battle Creek, MI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Code                            MetroArea\n",
       "0   460          Appleton-Oshkosh-Neenah, WI\n",
       "1  3000    Grand Rapids-Muskegon-Holland, MI\n",
       "2  3160  Greenville-Spartanburg-Anderson, SC\n",
       "3  3610                        Jamestown, NY\n",
       "4  3720           Kalamazoo-Battle Creek, MI"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metro = pd.read_csv('MetroAreaCodes.csv')\n",
    "df_metro.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(271, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metro.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many observations (codes for countries) are there in CountryMap?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66</td>\n",
       "      <td>Guam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73</td>\n",
       "      <td>Puerto Rico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>78</td>\n",
       "      <td>U. S. Virgin Islands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>96</td>\n",
       "      <td>Other U. S. Island Areas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Code                   Country\n",
       "0    57             United States\n",
       "1    66                      Guam\n",
       "2    73               Puerto Rico\n",
       "3    78      U. S. Virgin Islands\n",
       "4    96  Other U. S. Island Areas"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_countries = pd.read_csv('CountryCodes.csv')\n",
    "df_countries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_countries.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 3.2 - Integrating Metropolitan Area Data\n",
    "2 points possible (graded)\n",
    "To merge in the metropolitan areas, we want to connect the field MetroAreaCode from the CPS data frame with the field Code in MetroAreaMap. The following command merges the two data frames on these columns, overwriting the CPS data frame with the result:\n",
    "\n",
    "CPS = merge(CPS, MetroAreaMap, by.x=\"MetroAreaCode\", by.y=\"Code\", all.x=TRUE)\n",
    "\n",
    "The first two arguments determine the data frames to be merged (they are called \"x\" and \"y\", respectively, in the subsequent parameters to the merge function). by.x=\"MetroAreaCode\" means we're matching on the MetroAreaCode variable from the \"x\" data frame (CPS), while by.y=\"Code\" means we're matching on the Code variable from the \"y\" data frame (MetroAreaMap). Finally, all.x=TRUE means we want to keep all rows from the \"x\" data frame (CPS), even if some of the rows' MetroAreaCode doesn't match any codes in MetroAreaMap (for those familiar with database terminology, this parameter makes the operation a left outer join instead of an inner join).\n",
    "\n",
    "Review the new version of the CPS data frame with the summary() and str() functions. What is the name of the variable that was added to the data frame by the merge() operation?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PeopleInHousehold</th>\n",
       "      <th>Region</th>\n",
       "      <th>State</th>\n",
       "      <th>MetroAreaCode</th>\n",
       "      <th>Age</th>\n",
       "      <th>Married</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Education</th>\n",
       "      <th>Race</th>\n",
       "      <th>Hispanic</th>\n",
       "      <th>CountryOfBirthCode</th>\n",
       "      <th>Citizenship</th>\n",
       "      <th>EmploymentStatus</th>\n",
       "      <th>Industry</th>\n",
       "      <th>is_US_citizen</th>\n",
       "      <th>has_NAN</th>\n",
       "      <th>NAN_Married</th>\n",
       "      <th>Missing_MetroAreCode</th>\n",
       "      <th>MetroArea</th>\n",
       "      <th>Code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>South</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>26620.0</td>\n",
       "      <td>85</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Female</td>\n",
       "      <td>Associate degree</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>Citizen, Native</td>\n",
       "      <td>Retired</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Huntsville, AL</td>\n",
       "      <td>26620.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>South</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>13820.0</td>\n",
       "      <td>21</td>\n",
       "      <td>Never Married</td>\n",
       "      <td>Male</td>\n",
       "      <td>High school</td>\n",
       "      <td>Black</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>Citizen, Native</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>Professional and business services</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Birmingham-Hoover, AL</td>\n",
       "      <td>13820.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>South</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>13820.0</td>\n",
       "      <td>37</td>\n",
       "      <td>Never Married</td>\n",
       "      <td>Female</td>\n",
       "      <td>High school</td>\n",
       "      <td>Black</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>Citizen, Native</td>\n",
       "      <td>Disabled</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Birmingham-Hoover, AL</td>\n",
       "      <td>13820.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>South</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>13820.0</td>\n",
       "      <td>18</td>\n",
       "      <td>Never Married</td>\n",
       "      <td>Male</td>\n",
       "      <td>No high school diploma</td>\n",
       "      <td>Black</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>Citizen, Native</td>\n",
       "      <td>Not in Labor Force</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Birmingham-Hoover, AL</td>\n",
       "      <td>13820.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>South</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>26620.0</td>\n",
       "      <td>52</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Female</td>\n",
       "      <td>Associate degree</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>Citizen, Native</td>\n",
       "      <td>Employed</td>\n",
       "      <td>Professional and business services</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Huntsville, AL</td>\n",
       "      <td>26620.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PeopleInHousehold Region    State  MetroAreaCode  Age        Married  \\\n",
       "0                  1  South  Alabama        26620.0   85        Widowed   \n",
       "1                  3  South  Alabama        13820.0   21  Never Married   \n",
       "2                  3  South  Alabama        13820.0   37  Never Married   \n",
       "3                  3  South  Alabama        13820.0   18  Never Married   \n",
       "4                  3  South  Alabama        26620.0   52        Widowed   \n",
       "\n",
       "      Sex               Education   Race  Hispanic  CountryOfBirthCode  \\\n",
       "0  Female        Associate degree  White         0                  57   \n",
       "1    Male             High school  Black         0                  57   \n",
       "2  Female             High school  Black         0                  57   \n",
       "3    Male  No high school diploma  Black         0                  57   \n",
       "4  Female        Associate degree  White         0                  57   \n",
       "\n",
       "       Citizenship    EmploymentStatus                            Industry  \\\n",
       "0  Citizen, Native             Retired                                 NaN   \n",
       "1  Citizen, Native          Unemployed  Professional and business services   \n",
       "2  Citizen, Native            Disabled                                 NaN   \n",
       "3  Citizen, Native  Not in Labor Force                                 NaN   \n",
       "4  Citizen, Native            Employed  Professional and business services   \n",
       "\n",
       "  is_US_citizen has_NAN NAN_Married Missing_MetroAreCode  \\\n",
       "0          True    True       False                False   \n",
       "1          True   False       False                False   \n",
       "2          True    True       False                False   \n",
       "3          True    True       False                False   \n",
       "4          True   False       False                False   \n",
       "\n",
       "               MetroArea     Code  \n",
       "0         Huntsville, AL  26620.0  \n",
       "1  Birmingham-Hoover, AL  13820.0  \n",
       "2  Birmingham-Hoover, AL  13820.0  \n",
       "3  Birmingham-Hoover, AL  13820.0  \n",
       "4         Huntsville, AL  26620.0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demo_metro=pd.merge(df_demo,df_metro[['MetroArea','Code']],left_on='MetroAreaCode',right_on='Code',how='left')\n",
    "df_demo_metro.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many interviewees have a missing value for the new metropolitan area variable? Note that all of these interviewees would have been removed from the merged data frame if we did not include the all.x=TRUE parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34238, 20)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demo_metro[df_demo_metro.MetroArea.isnull()].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 3.3 - Integrating Metropolitan Area Data\n",
    "1 point possible (graded)\n",
    "Which of the following metropolitan areas has the largest number of interviewees?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MetroArea\n",
      "New York-Northern New Jersey-Long Island, NY-NJ-PA    5409\n",
      "Washington-Arlington-Alexandria, DC-VA-MD-WV          4177\n",
      "Los Angeles-Long Beach-Santa Ana, CA                  4102\n",
      "Philadelphia-Camden-Wilmington, PA-NJ-DE              2855\n",
      "Chicago-Naperville-Joliet, IN-IN-WI                   2772\n",
      "Providence-Fall River-Warwick, MA-RI                  2284\n",
      "Boston-Cambridge-Quincy, MA-NH                        2229\n",
      "Minneapolis-St Paul-Bloomington, MN-WI                1942\n",
      "Dallas-Fort Worth-Arlington, TX                       1863\n",
      "Houston-Baytown-Sugar Land, TX                        1649\n",
      "Honolulu, HI                                          1576\n",
      "Miami-Fort Lauderdale-Miami Beach, FL                 1554\n",
      "Atlanta-Sandy Springs-Marietta, GA                    1552\n",
      "Denver-Aurora, CO                                     1504\n",
      "Baltimore-Towson, MD                                  1483\n",
      "San Francisco-Oakland-Fremont, CA                     1386\n",
      "Detroit-Warren-Livonia, MI                            1354\n",
      "Las Vegas-Paradise, NV                                1299\n",
      "Riverside-San Bernardino, CA                          1290\n",
      "Seattle-Tacoma-Bellevue, WA                           1255\n",
      "Portland-Vancouver-Beaverton, OR-WA                   1089\n",
      "Phoenix-Mesa-Scottsdale, AZ                            971\n",
      "Kansas City, MO-KS                                     962\n",
      "Omaha-Council Bluffs, NE-IA                            957\n",
      "St. Louis, MO-IL                                       956\n",
      "San Diego-Carlsbad-San Marcos, CA                      907\n",
      "Hartford-West Hartford-East Hartford, CT               885\n",
      "Tampa-St. Petersburg-Clearwater, FL                    842\n",
      "Pittsburgh, PA                                         732\n",
      "Bridgeport-Stamford-Norwalk, CT                        730\n",
      "                                                      ... \n",
      "Farmington, NM                                          64\n",
      "Florence, AL                                            63\n",
      "Monroe, MI                                              63\n",
      "Lubbock, TX                                             63\n",
      "Johnstown, PA                                           63\n",
      "Jacksonville, NC                                        63\n",
      "Anderson, IN                                            62\n",
      "Napa, CA                                                61\n",
      "Anniston-Oxford, AL                                     61\n",
      "Chico, CA                                               60\n",
      "Panama City-Lynn Haven, FL                              59\n",
      "Columbus, GA-AL                                         59\n",
      "Joplin, MO                                              59\n",
      "Madera, CA                                              57\n",
      "Hickory-Morgantown-Lenoir, NC                           57\n",
      "Vineland-Millville-Bridgeton, NJ                        54\n",
      "Prescott, AZ                                            54\n",
      "Santa Fe, NM                                            52\n",
      "Johnson City, TN                                        52\n",
      "Niles-Benton Harbor, MI                                 51\n",
      "Midland, TX                                             51\n",
      "Punta Gorda, FL                                         48\n",
      "Columbia, MO                                            47\n",
      "Tallahassee, FL                                         43\n",
      "Warner Robins, GA                                       42\n",
      "Valdosta, GA                                            42\n",
      "Bloomington-Normal IL                                   40\n",
      "Springfield, OH                                         34\n",
      "Ocean City, NJ                                          30\n",
      "Bowling Green, KY                                       29\n",
      "Name: PeopleInHousehold, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print df_demo_metro.groupby('MetroArea').PeopleInHousehold.count().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 3.4 - Integrating Metropolitan Area Data\n",
    "2.0 points possible (graded)\n",
    "Which metropolitan area has the highest proportion of interviewees of Hispanic ethnicity? Hint: Use tapply() with mean, as in the previous subproblem. Calling sort() on the output of tapply() could also be helpful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_demo_metro_hispanic = df_demo_metro.groupby(['MetroArea','Hispanic']).PeopleInHousehold.count().unstack('Hispanic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_demo_metro_hispanic['Ratio']=df_demo_metro_hispanic[1]*100.0/df_demo_metro_hispanic.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Laredo, TX'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demo_metro_hispanic.Ratio.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 3.5 - Integrating Metropolitan Area Data\n",
    "2.0 points possible (graded)\n",
    "Remembering that CPS$Race == \"Asian\" returns a TRUE/FALSE vector of whether an interviewee is Asian, determine the number of metropolitan areas in the United States from which at least 20% of interviewees are Asian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_demo_metro_asian = df_demo_metro\n",
    "df_demo_metro_asian['Asian']=df_demo_metro_asian.Race=='Asian'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "groupyAsian = df_demo_metro_asian.groupby(['MetroArea','Asian']).PeopleInHousehold.count().unstack('Asian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "groupyAsian['Ratio']=groupyAsian[True]*100.0/groupyAsian.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Asian</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "      <th>Ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MetroArea</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Honolulu, HI</th>\n",
       "      <td>785.0</td>\n",
       "      <td>791.0</td>\n",
       "      <td>49.148207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>San Francisco-Oakland-Fremont, CA</th>\n",
       "      <td>1044.0</td>\n",
       "      <td>342.0</td>\n",
       "      <td>24.327929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>San Jose-Sunnyvale-Santa Clara, CA</th>\n",
       "      <td>508.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>23.496269</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Asian                                False   True      Ratio\n",
       "MetroArea                                                   \n",
       "Honolulu, HI                         785.0  791.0  49.148207\n",
       "San Francisco-Oakland-Fremont, CA   1044.0  342.0  24.327929\n",
       "San Jose-Sunnyvale-Santa Clara, CA   508.0  162.0  23.496269"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupyAsian[groupyAsian.Ratio>=20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 3.6 - Integrating Metropolitan Area Data\n",
    "1 point possible (graded)\n",
    "Normally, we would look at the sorted proportion of interviewees from each metropolitan area who have not received a high school diploma with the command:\n",
    "\n",
    "sort(tapply(CPS$Education == \"No high school diploma\", CPS$MetroArea, mean))\n",
    "\n",
    "However, none of the interviewees aged 14 and younger have an education value reported, so the mean value is reported as NA for each metropolitan area. To get mean (and related functions, like sum) to ignore missing values, you can pass the parameter na.rm=TRUE. Passing na.rm=TRUE to the tapply function, determine which metropolitan area has the smallest proportion of interviewees who have received no high school diploma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_demo_metro_no_highschool = df_demo_metro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "groupy_no_highschool = df_demo_metro_no_highschool.groupby(['MetroArea','Education']).PeopleInHousehold.count().unstack('Education')\n",
    "#groupy_no_highschool['Ratio']=groupy_no_highschool[True]*100.0/groupy_no_highschool.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "groupy_no_highschool['Ratio_No_Highschool']=groupy_no_highschool['No high school diploma']*100.0/groupy_no_highschool.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Iowa City, IA'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupy_no_highschool['Ratio_No_Highschool'].argmin()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 4.1 - Integrating Country of Birth Data\n",
    "2 points possible (graded)\n",
    "Just as we did with the metropolitan area information, merge in the country of birth information from the CountryMap data frame, replacing the CPS data frame with the result. If you accidentally overwrite CPS with the wrong values, remember that you can restore it by re-loading the data frame from CPSData.csv and then merging in the metropolitan area information using the command provided in the previous subproblem.\n",
    "\n",
    "What is the name of the variable added to the CPS data frame by this merge operation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_demo_metro_birth = pd.merge(df_demo_metro,df_countries,left_on='CountryOfBirthCode',right_on='Code',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'PeopleInHousehold', u'Region', u'State', u'MetroAreaCode', u'Age',\n",
       "       u'Married', u'Sex', u'Education', u'Race', u'Hispanic',\n",
       "       u'CountryOfBirthCode', u'Citizenship', u'EmploymentStatus', u'Industry',\n",
       "       u'is_US_citizen', u'has_NAN', u'NAN_Married', u'Missing_MetroAreCode',\n",
       "       u'MetroArea', u'Code_x', u'Asian', u'No High school diploma', u'Code_y',\n",
       "       u'Country'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demo_metro_birth.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many interviewees have a missing value for the new country of birth variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(176, 24)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demo_metro_birth[df_demo_metro_birth.Country.isnull()].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 4.2 - Integrating Country of Birth Data\n",
    "2.0 points possible (graded)\n",
    "Among all interviewees born outside of North America, which country was the most common place of birth?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country\n",
       "United States    115063\n",
       "Mexico             3921\n",
       "Philippines         839\n",
       "India               770\n",
       "China               581\n",
       "Name: PeopleInHousehold, dtype: int64"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demo_metro_birth.groupby('Country').count().PeopleInHousehold.sort_values(ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 4.3 - Integrating Country of Birth Data\n",
    "2.0 points possible (graded)\n",
    "What proportion of the interviewees from the \"New York-Northern New Jersey-Long Island, NY-NJ-PA\" metropolitan area have a country of birth that is not the United States? For this computation, don't include people from this metropolitan area who have a missing country of birth.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.866025166543302"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_4_3 =df_demo_metro_birth[(df_demo_metro_birth.MetroArea=='New York-Northern New Jersey-Long Island, NY-NJ-PA')].Country.value_counts()\n",
    "df_4_3[1:].sum()*100.0/df_4_3.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 4.4 - Integrating Country of Birth Data\n",
    "3 points possible (graded)\n",
    "Which metropolitan area has the largest number (note -- not proportion) of interviewees with a country of birth in India? Hint -- remember to include na.rm=TRUE if you are using tapply() to answer this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'New York-Northern New Jersey-Long Island, NY-NJ-PA'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demo_metro_birth[df_demo_metro_birth.Country=='India'].groupby('MetroArea').count().PeopleInHousehold.argmax()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
