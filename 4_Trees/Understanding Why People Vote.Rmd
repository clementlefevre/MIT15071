---
title: "Understanding why people vote"
output:
  html_document:
    keep_md: true
---




```{r}
library(dplyr)
library(tidyr)
library(caret)
library(ROCR)
library(caTools)
library(rpart)
library(rpart.plot)

```
Problem 1.1 - Exploration and Logistic Regression
1 point possible (graded)
We will first get familiar with the data. Load the CSV file gerber.csv into R. What proportion of people in this dataset voted in this election?
```{r}
df<-read.csv('gerber.csv')
df<-tibble::rownames_to_column(data.frame(df),'indexo')
df$indexo <- as.integer(df$indexo)
head(df)
sum(df$voting==1)/nrow(df)
```
Problem 1.2 - Exploration and Logistic Regression
1 point possible (graded)
Which of the four "treatment groups" had the largest percentage of people who actually voted (voting = 1)?


Civic Duty
Hawthorne Effect
Self
Neighbors
```{r}
per_group <-df %>% gather("group","is_group",5:8) %>% filter(is_group>0)
per_group  %>% arrange(indexo) %>% group_by(group,voting) %>% summarise(total = n()) %>% spread(voting,total) %>% setNames(c('group',"did_not_vote", "voted")) %>% mutate(ratio = voted/(did_not_vote+voted)*100) %>% arrange(ratio)
```
Problem 1.3 - Exploration and Logistic Regression
1 point possible (graded)
Build a logistic regression model for voting using the four treatment group variables as the independent variables (civicduty, hawthorne, self, and neighbors). Use all the data to build the model (DO NOT split the data into a training set and testing set). Which of the following coefficients are significant in the logistic regression model? Select all that apply.


Civic Duty
Hawthorne Effect
Self
Neighbors
```{r}
logModel1 <- glm(voting~civicduty+hawthorne+self+neighbors, data=df, family = binomial)
summary(logModel1)
CM<-table(df$voting,predict(logModel1,df,type='response')>0.3)
sum(diag(CM))/sum(CM)
CM<-table(df$voting,predict(logModel1,df,type='response')>0.5)
sum(diag(CM))/sum(CM)
```
Compute Baselin
```{r}

ROCRpred = prediction(predict(logModel1,df,type='response'), df$voting)
as.numeric(performance(ROCRpred, "auc")@y.values)

# Performance function
ROCRperf = performance(ROCRpred, "tpr", "fpr")


# Add threshold labels 
plot(ROCRperf, colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.1), text.adj=c(-0.2,1.7))
```
Problem 2.1 - Trees
1 point possible (graded)
We will now try out trees. Build a CART tree for voting using all data and the same four treatment variables we used before. Don't set the option method="class" - we are actually going to create a regression tree here. We are interested in building a tree to explore the fraction of people who vote, or the probability of voting. We’d like CART to split our groups if they have different probabilities of voting. If we used method=‘class’, CART would only split if one of the groups had a probability of voting above 50% and the other had a probability of voting less than 50% (since the predicted outcomes would be different). However, with regression trees, CART will split even if both groups have probability less than 50%.

Leave all the parameters at their default values. You can use the following command in R to build the tree:

CARTmodel = rpart(voting ~ civicduty + hawthorne + self + neighbors, data=gerber)
Plot the tree. What happens, and if relevant, why?


```{r}

```

